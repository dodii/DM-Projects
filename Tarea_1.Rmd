---
title: "Tarea 1: Exploración y Visualización de Datos"
author: "Felipe Bravo, Hernán Sarmiento, Aymé Arango, Alison Fernandez, Cinthia Mabel Sanchez, Juan Pablo Silva"
date: "Septiembre 2020"
output:
  html_document:
    number_sections: yes
    theme: spacelab
    toc: yes
  pdf_document:
    toc: yes
---

# Declaración de compromiso ético
Nosotros Francisco Molina y Rodrigo Oportot, declaramos que realizamos de manera grupal los pasos de la presente actividad. También declaramos no incurrir en copia, ni compartir nuestras respuestas con otras personas ni con otros grupos. Por lo que, ratificamos que las respuestas son de nuestra propia confección y reflejan nuestro propio conocimiento.

# Instrucciones

1. Trabajen en equipos de dos o tres personas. Salvo excepciones, no se corregirá entregas con menos de dos integrantes.

2. Modifique este archivo `.Rmd` agregando sus respuestas donde corresponda.

3. Para cada pregunta, cuando corresponda, **incluya el código fuente que utilizó para llegar a su respuesta**.

4. El formato de entrega para esta actividad es un archivo html. **Genere un archivo HTML usando RStudio** y súbalo a U-Cursos.
   Basta con que uno de los integrantes haga la entrega. Si ambos hacen una entrega en U-Cursos, se revisará cualquiera de éstas.

# Tarea 
La primera parte de esta actividad son preguntas teóricas que avanzaron en las clases del curso de Minería de datos.

## Teoría

*1. ¿Cuál es el objetivo de Minería de datos y qué la diferencia de Machine Learning? De un ejemplo.*

**Respuesta: El objetivo principal de la minería de datos es descubrir automáticamente información útil en grandes cantidades de datos, mediante diversas herramientas de varias disciplinas relativamente relacionadas, tales como la estadística, el reconocimiento de patrones en bases de datos que escapen de la magnitud abordable por el ojo humano o el almacenamiento y normalización de bases de datos. Mediante DM, se busca descubrir información útil, extrayendo conocimiento a través de automatizaciones (que pueden ser desarrolladas a partir de ML), permitiendo comprender dicha información de forma mucho más directa una vez ésta sea analizada por una persona. Por otro lado, podemos diferenciarla de Machine Learning en que éste último tiene como fin generar predicciones o preveer resultados esperables en base a datos e información previa, mediante el diseño, desarrollo y entrenamiento de algoritmos sin involucrarse directamente en el tipo de análisis que DM persigue de antemano, ya que dichos algoritmos pueden ser utilizados sobre distintos tipos de datos.**


*2. Describa y compare los siguientes métodos usados en Minería de datos: clasificación vs. clustering.*

**Respuesta: En primera instancia, podemos diferenciar entre métodos descriptivos y métodos predictivos: el primero busca encontrar patrones reconocibles e interpretables por humanos para describir los datos, mientras que el segundo busca predecir variables o datos desconocidas y/o futuras en base a información ya conocida.**

**La clasificación corresponde a un método predictivo, se toma un set de entrenamiento que incluye las etiquetas de clases y tiene por objetivo asignarle una clase a un nuevo elemento. Por otra parte, el clustering es un método descriptivo, que busca encontrar conjuntos de datos que sean similares entre sí, sin utilizar un conjunto de entrenamiento.**


*3. ¿Qué desafios existen en Minería de datos?*

**
Respuesta:
Hoy en día la minería de datos tiene varios desafíos: 
* Escalabilidad: Los modelos de DM deben funcionar tanto en baja escala como en gran escala.
* Dimensionalidad: Se debe encontrar un punto de equilibrio entre trabajar con datasets de muchas dimensiones y de muy pocas dimensiones. Mientras más dimensiones tenga, más costoso es de trabajar, mientras que al tener menos dimensiones, más difícil es realizar análisis sobre él o encontrar patrones.
* Datos complejos y heterogéneos: Se debe lidiar con datos que pueden no tener una estructura definida (complejos) y datos que de por sí sean muy distintos y que no permitan encontrar patrones (heterogéneos).
* Privacidad: Qué tan privada puede llegar a ser la información presente en los datos públicos.
* Calidad: Al trabajar con datos de la red usualmente se presentan muchas dificultades con respecto a su calidad: no están bien estructurados, no vienen separados de la manera correcta, faltan datos, hay desbalances, etc...
* Streaming: Muchos de los datos con los que se trabaja hoy en día provienen de grandes volúmenes de datos que no tienen estructura alguna (YouTube, Facebook, Twitter), por lo que se vuelve costoso trabajar con ellos.
**

*4. Respecto a los tipos de atributo, ¿cuál es la diferencia entre razón e intervalo?*

**Respuesta:
Tanto los datos tipo razón como intervalo corresponden a tipos de datos cuantitativos. Su diferencia fundamental corresponde a que los datos del tipo intervalo no tienen definido un cero absoluto, mientras que los de razón sí. Ejemplos de datos tipo intervalo son: las fechas, temperatura en Celsius y temperatura en Farenheit, pues ninguno de estos datos tiene definido un cero absoluto. Por otra parte, ejemplos de datos tipo razón son: la hora, el largo y la temperatura en Kelvin, pues todos estos sí tienen definido un cero absoluto.
**

*5. ¿Qué factores que ocasionan errores en el análisis de datos deben ser considerados para la limpieza de un set de datos?*

**Respuesta:
El ruido (que corresponde a una componente aleatoria en la medición), los outliers o valores atípicos(objetos con características considerablemente diferentes a la mayoría), los valores faltantes (que pueden provenir de información no recolectada o atributos no aplicables a todos los grupos) y los datos duplicados.
**

*6. ¿Qué es el análisis exploratorio de datos o EDA?*

**Respuesta: El análisis exploratorio de datos (EDA) es un conjunto de técnicas y herramientas empleadas en ciencias de los datos para analizar y comprender de manera eficiente y rápida la estructura y composición de un conjunto particular de datos sujetos a estudio. Se basa principalmente en resumir los datos mediante herramientas estadísticas para luego visualizarlos en herramientas gráficas que permitan una mejor comprensión de la información**


*7. Describa las medidas de tendencia central: media y mediana. Exponga la diferencia entre ambas.*

**Respuesta: 
Las medidas de tendencia central tratan de resumir los valores observados en un único valor asociado al valor localizado en el centro. La media corresponde al promedio aritmético entre los datos medidos, mientras que la mediana representa al dato que separa la mitad inferior de la mitad superior de las observaciones (está justo en el centro). Estas medidas suelen estar en valores muy cercanos entre sí, aunque no hay que fiarse de que éste será siempre el caso: la media es muy sensible a los outliers, mientras que la mediana no.
**

*8. ¿Qué es una matriz de correlación y para qué sirve?*

**Respuesta: Una matriz de correlación es una herramienta matemática construida a partir de una matriz de dimensiones cuadradas, donde en sus filas se colocan una parte de los datos y en las columnas otra parte de ellos, comparándolos mediante un valor númerico real conocido como coeficiente de correlación de Pearson r(x,y) = cov(x,y)/(sd(x)sd(y)), con valores entre [-1, 1] para cada posición en la matriz (notar que r(x,x) = 1). Dependiendo del valor, se pueden notar distintas correlaciones entre los elementos: un valor negativo lejano a 0 implica una proporcionalidad inversa entre ellos; un valor en magnitud absoluta cercano a 0 implica ausencia de correlación lineal; un valor positivo lejano a 0 implica una proporcionalidad lineal directa. Es importante señalar que, aunque no haya correlación lineal, esto no implica que no existan otros tipos de correlaciones entre los datos (dígase exponencial o multiplicativa, por ejemplo). **

*9. Explique cómo se construye un Boxplot y exponga cómo se identifican los valores atípicos u outliers en este tipo de gráfico.*

**Respuesta: 
Los boxplots se construyen a partir de los percentiles. Se construye un rectángulo usando el primer y tercer cuartil como límites, de modo que la altura del rectángulo corresponde al rango inter-cuartil RIC; luego se traza una línea que divide al rectángulo en dos, correspondiente a la mediana; y finalmente se extiende cada extremo del rectángulo con brazos de largo Q1 - 1.5RIC para la inferior y Q3 + 1.5RIC para el superior. Los valores atípicos son aquellos que quedan fuera tanto del rectángulo como de sus brazos.
**


##----------------------------------------------------------------------------------------------------------------------------------------##

## Práctica 

En esta parte de la actividad se trabajará con los datos del Proceso Constituyente 2016-2017 publicados en el Portal de Datos Abiertos del Gobierno de Chile, para mayor información pueden ingresar al siguiente link: https://datos.gob.cl/dataset/proceso-constituyente-abierto-a-la-ciudadania. Los datos corresponden a las actas de los Encuentros Locales Autoconvocados (ELAs), en cada cual, un grupo de personas se reune a discutir distintos conceptos como por ejemplo: salud, educación, seguridad, etc.

Los datos con los que trabajarán consisten en la cantidad de veces que cada concepto constitucional fue mencionado por cada localidad de Chile. 

Para cargar los datos, use:

```{r}
data_tf <- read.csv("http://dcc.uchile.cl/~hsarmien/mineria/datasets/actas.txt", header = T)
```

**Por cada pregunta adjunte el código R que utilizó para llegar a la respuesta. Respuestas sin código no recibirán puntaje**

### Exploración básica

1. ¿Cuáles son las dimensiones del dataset (filas, columnas)? Adjunte código o indique cómo determinó la cantidad de datos total. 

El dataset "data_tf" tiene 328 filas y 113 columnas.
```{r}
dim(data_tf)
```

2. ¿Qué describe cada línea del dataset? (ejemplifique tomando la fila 85)


Cada fila describe los resultados de cada encuentro local autoconvocado, cada columna representa la cantidad de veces que apareció un concepto respondiendo a alguna de las preguntas de la ELA por parte de los ciudadanos. En particular, la fila 85 muestra los resultados del ELA en la ciudad de Andacollo, ubicada en la provincia de Elqui, perteneciente a la región de Coquimbo; y a primera vista se observa que se mencionaron conceptos como derechos: la educación, la integridad física y psiquica, la nacionalidad, la salud, etc...
```{r}
data_tf[85,]
```

3. ¿Existen localidades repetidas en el dataset? Adjunte el código o indique cómo llegó a esa conclusión. 

En la pregunta (1) se estableció que el dataset tiene 328 filas, y la función unique muestra 328 resultados distintos para la categoría "localidad". Por lo tanto no hay localidades repetidas.
```{r}
unique(data_tf["localidad"])
```

4. Liste los nombres de las columnas del dataset `data_tf`. Adjunte código en R y *recuerde* usar `head` si el resultado es muy largo.

Dado que hay 113 columnas, se muestran las primeras 6 usando la función head:

```{r}
head(colnames(data_tf))
```


### Análisis

1. Liste todas las localidades donde *no* se discutió el concepto `a_la_salud`. 

En sólo 17 localidades no fue discutido el concepto "a_la_salud":
```{r}
data_tf[data_tf$a_la_salud == 0,c("localidad")]
```

2. Liste las 10 localidades que más mencionaron el concepto `justicia`. 

Se obtuvieron ordenando según la columna justicia de modo descendiente:
```{r}
data_tf[order(-data_tf$justicia),c("localidad","justicia")][0:10,]
```


3. Liste los 10 conceptos menos mencionados a lo largo de todo el proceso.

Para poder sumar las filas por columnas se deja afuera la columna de "localidad", llamando a data_tf[2:113], luego se aplica la función colSums, que realiza sumatorias en las columnas restantes. Finalmente se aplica la función sort sobre lo anterior y se limita a 10 resultados, obteniéndose así los 10 conceptos mencionados a lo larto del proceso junto a su cantidad de apariciones: 
```{r}
sort(colSums(data_tf[2:113]))[0:10]
```

4. Liste las 10 localidades que más participaron en el proceso. Describa cómo definió su medida de participación.



```{r}
# RESPUESTA
```

5. Ejecute el siguiente código que permitirá agregar una nueva columna a nuestro dataframe que solo tendrá el nombre de la región.

```{r, message = F, warning=F}
library(dplyr)
regiones <- strsplit(as.character(data_tf[,1]), '/')
data_tf$region <- sapply(regiones, "[[", 1)
data_tf <- data_tf %>% select(localidad, region, everything())
```

Luego, genere un gráfico de barras (ggplot) que muestre los 10 conceptos más mencionados en cada una de las regiones mencionadas (adjunte gráficos y código):

- `Atacama`
- `Coquimbo`
- `Metropolitana de Santiago`


Cabe resaltar, que se esperan tres gráficos de barras para las tres diferentes regiones:

```{r}
library(stringr)
texto = function(x){
  str_to_title(str_replace_all(x,"_", "\n"))
}
```

```{r}
# 10 conceptos más mencionados en Atacama
topAtacama = as.data.frame(sort(colSums(data_tf[data_tf$region == "Atacama",][3:113]),decreasing = TRUE)[0:10])
colnames(topAtacama) = c("Apariciones") 

library(ggplot2)

ggplot(topAtacama) +   # asociamos un data frame a ggplot
  geom_bar(aes(x = reorder(rownames(topAtacama),Apariciones), y = Apariciones), stat="identity", fill = "#3fa31a", space = 3) +   # creamos un grafico de barras como una capa
  #coord_flip() +  # transformamos el grafico invirtiendo los ejes de coordenadas (sólo visualmente)
  ggtitle("10 conceptos más mencionados en Atacama") + # título
  xlab("Concepto") + ylab("Cantidad") + # etiquetas
  scale_x_discrete(labels = texto)
```

```{r}
# 10 conceptos más mencionados en Coquimbo
topCoquimbo = as.data.frame(sort(colSums(data_tf[data_tf$region == "Coquimbo",][3:113]),decreasing = TRUE)[0:10])
colnames(topCoquimbo) = c("Apariciones") 

library(ggplot2)

ggplot(topCoquimbo) +   # asociamos un data frame a ggplot
  geom_bar(aes(x = reorder(rownames(topCoquimbo),Apariciones), y = Apariciones), stat="identity", fill = "#e31010", space = 3) +   # creamos un grafico de barras como una capa
  #coord_flip() +  # transformamos el grafico invirtiendo los ejes de coordenadas (sólo visualmente)
  ggtitle("10 conceptos más mencionados en Coquimbo") + # título
  xlab("Concepto") + ylab("Cantidad") + # etiquetas
  scale_x_discrete(labels = texto)
```

```{r}
# 10 conceptos más mencionados en Santiago
topSantiago = as.data.frame(sort(colSums(data_tf[data_tf$region == "Metropolitana de Santiago",][3:113]),decreasing = TRUE)[0:10])
colnames(topSantiago) = c("Apariciones") 

library(ggplot2)

ggplot(topSantiago) +   # asociamos un data frame a ggplot
  geom_bar(aes(x = reorder(rownames(topSantiago),Apariciones), y = Apariciones), stat="identity", fill = "#0f1af2", space = 3) +   # creamos un grafico de barras como una capa
  #coord_flip() +  # transformamos el grafico invirtiendo los ejes de coordenadas (sólo visualmente)
  ggtitle("10 conceptos más mencionados en Santiago") + # título
  xlab("Concepto") + ylab("Cantidad") + # etiquetas
  scale_x_discrete(labels = texto) 
```

6. De la pregunta anterior, ¿considera que es razonable usar el conteo de frecuencias para determinar las regiones que tuvieron mayor participación en el proceso? ¿Por qué? Sugiera y solamente comente una forma distinta de hacerlo.

```{r}
# RESPUESTA
```

## Ejercicios

### Accidentes de tránsito

Para esta sección utilizaremos un dataset real de número de accidentes de tránsito por localidad, el cual puede ser encontrado en el siguiente link: http://datos.gob.cl/dataset/9348. Para cargar el dataset ejecute el siguiente código:

```{r}
tipos <- read.table("https://users.dcc.uchile.cl/~hsarmien/mineria/datasets/accidentes_2010_2011.txt")
head(tipos)
```

Explore el set de datos para responder las siguientes preguntas:

1. Filtre los datos para incluir sólo los accidentes ocurridos el año 2011 a nivel regional. Genere un boxplot donde se indique la cantidad de accidentes categorizado por tipo de accidente.

Este tipo de gráfico nos ayudará a entender como se distribuye los datos por cada tipo de accidentes. Es decir, podremos apreciar que tan dispersos o similares son los datos en todo el dataset. También, puede ser útil para observar valores atípicos u outliers en los datos.


Se ajustaron los límites del eje Y para ignorar a los outliers que distorsionaban la escala:
```{r}
regional_2011 = tipos[tipos$Anio == 2011 & tipos$Muestra == "Regional",]
ggplot(regional_2011, aes(TipoAccidente,Cantidad))+
  geom_boxplot(notch=FALSE, fill = "#36c6e3")+
  coord_cartesian(ylim = c(0, 4000))
```

2. ¿Qué aprecia con el gráfico anterior? ¿Qué otra forma de explorar los datos podría agregar? ¿Qué información adicional aporta? Adjunte una breve explicación.

**Respuesta: Se observan los distintos tipos de accidentes de tránsito y sus cantidades en función de lo pedido en el enunciado. Se puede notar que la mayoría de los accidentes corresponden a colisiones entre vehículos. Al analizar los datos en su forma matricial (filas y columnas), podemos observar que la Región Metropolitana acapara las mayores cantidades en los distintos tipos de accidentes; esto puede deberse a la mayor concentración y número de habitantes y vehículos, en comparación con las otras regiones del país. No obstante, junto con responder la segunda pregunta, hay otros factores por considerar para explorar nuevamente los datos: la proporción de accidentes en función del número de habitantes por cada región, ya que se evidencia cómo la RM altera significativamente la información gráfica al actuar como un outlier, por lo que se podría explorar el dataset apartando a la RM en un análisis separado. Hay que tener en cuenta que otro factor de suma importancia en los accidentes de tránsito es la calidad, competencia y responsabilidad de los conductores, dependiendo esto del grado de fiscalización con que se evalúen los exámenes de conducir, así como el cumplimiento de las leyes de tránsito dentro de las regiones. Una mejor fiscalización y multas más severas para los infractores puede reducir el número de accidentes en el lugar donde dichas condiciones se apliquen, afectando así los datos. Con todo esto en cuenta, si se analiza la RM por separado del resto de las regiones, considerando sus proporciones de ciudadanos y/o vehículos, podrían observarse diferencias sustanciales: quizá en la RM los ciudadanos "conducen mejor" porque la proporción de accidentes en función de ciudadanos y/o vehículos es menor (recalcando que esto es una suposición), sustentándose en el hipotético hecho de que se fiscaliza con más intensidad en tal región, entre otras posibles observaciones que puedan surgir. **

### Diamantes

Considere el set de datos diamonds del paquete ggplot2 de R, que contiene los precios en dolares, junto con otros atributos importantes: quilates, corte, color y claridad. También hay medidas físicas como ser: x (largo), y (ancho), z (profundidad), depth (porcentaje total de profundidad) y table (ancho desde el tope del diamante al punto relativo más ancho del diamante):

```{r}
library(ggplot2)
data("diamonds")
head(diamonds)
```

Realice una exploración por el set de datos para responder las siguientes preguntas:

1. Teniendo en cuenta las medidas físicas, ¿considera que existen valores inexistentes o inconsistentes? Describa como manejaría esta situación. Adjunte el código necesario.

**Respuesta: **

```{r}
# RESPUESTA

```

2. Considerando la relación entre dos atributos, ¿qué atributos están más correlacionadas con el precio (price) y qué significa esto? ¿cuál es la correlación más alta para table? Adjunte el código necesario para la respuesta.

```{r}
# RESPUESTA
```

3. Proponga otra forma para explorar los datos. ¿Qué información adicional aporta? Adjunte una breve explicación.

**Respuesta:**